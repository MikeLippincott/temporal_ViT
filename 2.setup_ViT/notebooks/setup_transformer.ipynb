{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "sys.path.append(\"../helpers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500000, 1)\n",
      "(1500000, 384)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.029546</td>\n",
       "      <td>-0.093779</td>\n",
       "      <td>0.035240</td>\n",
       "      <td>-0.029979</td>\n",
       "      <td>0.094636</td>\n",
       "      <td>0.060090</td>\n",
       "      <td>-0.011613</td>\n",
       "      <td>-0.011853</td>\n",
       "      <td>0.025654</td>\n",
       "      <td>0.051191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016053</td>\n",
       "      <td>0.055923</td>\n",
       "      <td>-0.015797</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.090052</td>\n",
       "      <td>0.084351</td>\n",
       "      <td>0.031153</td>\n",
       "      <td>0.027775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.029816</td>\n",
       "      <td>-0.066288</td>\n",
       "      <td>0.014838</td>\n",
       "      <td>-0.033962</td>\n",
       "      <td>0.069943</td>\n",
       "      <td>0.035519</td>\n",
       "      <td>-0.034434</td>\n",
       "      <td>-0.038205</td>\n",
       "      <td>0.018258</td>\n",
       "      <td>0.085273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024541</td>\n",
       "      <td>0.068040</td>\n",
       "      <td>-0.034506</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.047106</td>\n",
       "      <td>-0.011689</td>\n",
       "      <td>0.088033</td>\n",
       "      <td>0.080813</td>\n",
       "      <td>0.039639</td>\n",
       "      <td>0.052613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.020296</td>\n",
       "      <td>0.022372</td>\n",
       "      <td>-0.021896</td>\n",
       "      <td>-0.049170</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>-0.052099</td>\n",
       "      <td>-0.106442</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.093113</td>\n",
       "      <td>0.024022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.067288</td>\n",
       "      <td>0.048709</td>\n",
       "      <td>-0.005579</td>\n",
       "      <td>-0.033400</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>-0.011651</td>\n",
       "      <td>0.063168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.020296</td>\n",
       "      <td>0.022372</td>\n",
       "      <td>-0.021896</td>\n",
       "      <td>-0.049170</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>-0.052099</td>\n",
       "      <td>-0.106442</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.093113</td>\n",
       "      <td>0.024022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.067288</td>\n",
       "      <td>0.048709</td>\n",
       "      <td>-0.005579</td>\n",
       "      <td>-0.033400</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>-0.011651</td>\n",
       "      <td>0.063168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003252</td>\n",
       "      <td>0.024647</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>-0.039569</td>\n",
       "      <td>0.053363</td>\n",
       "      <td>-0.028918</td>\n",
       "      <td>-0.091161</td>\n",
       "      <td>-0.037333</td>\n",
       "      <td>0.029517</td>\n",
       "      <td>0.046394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>0.059754</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.005610</td>\n",
       "      <td>-0.011385</td>\n",
       "      <td>0.026064</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.067664</td>\n",
       "      <td>-0.016873</td>\n",
       "      <td>0.057681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.029546 -0.093779  0.035240 -0.029979  0.094636  0.060090 -0.011613   \n",
       "1 -0.029816 -0.066288  0.014838 -0.033962  0.069943  0.035519 -0.034434   \n",
       "2 -0.020296  0.022372 -0.021896 -0.049170  0.018540 -0.052099 -0.106442   \n",
       "3 -0.020296  0.022372 -0.021896 -0.049170  0.018540 -0.052099 -0.106442   \n",
       "4 -0.003252  0.024647  0.005446 -0.039569  0.053363 -0.028918 -0.091161   \n",
       "\n",
       "        7         8         9    ...       374       375       376       377  \\\n",
       "0 -0.011853  0.025654  0.051191  ...  0.016053  0.055923 -0.015797  0.009030   \n",
       "1 -0.038205  0.018258  0.085273  ...  0.024541  0.068040 -0.034506  0.004123   \n",
       "2  0.001228  0.093113  0.024022  ...  0.011164  0.067288  0.048709 -0.005579   \n",
       "3  0.001228  0.093113  0.024022  ...  0.011164  0.067288  0.048709 -0.005579   \n",
       "4 -0.037333  0.029517  0.046394  ...  0.024138  0.059754 -0.002547 -0.005610   \n",
       "\n",
       "        378       379       380       381       382       383  \n",
       "0  0.016759  0.004438  0.090052  0.084351  0.031153  0.027775  \n",
       "1  0.047106 -0.011689  0.088033  0.080813  0.039639  0.052613  \n",
       "2 -0.033400  0.029852  0.000440  0.026421 -0.011651  0.063168  \n",
       "3 -0.033400  0.029852  0.000440  0.026421 -0.011651  0.063168  \n",
       "4 -0.011385  0.026064  0.008469  0.067664 -0.016873  0.057681  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "cls_file_path = pathlib.Path(\n",
    "    \"../../1.scDINO_analysis/1.scDINO_run/outputdir/mnist_photos/CLS_features/channel_binary_model_dino_deitsmall16_pretrain_full_checkpoint_features.csv\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "image_paths_file_path = pathlib.Path(\n",
    "    \"../../1.scDINO_analysis/1.scDINO_run/outputdir/mnist_photos/CLS_features/image_paths.csv\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "# load in the image paths\n",
    "image_paths = pd.read_csv(image_paths_file_path, header=None)\n",
    "print(image_paths.shape)\n",
    "\n",
    "# load in the the data to a csv\n",
    "cls_features = pd.read_csv(cls_file_path, header=None)\n",
    "print(cls_features.shape)\n",
    "cls_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLS_0</th>\n",
       "      <th>CLS_1</th>\n",
       "      <th>CLS_2</th>\n",
       "      <th>CLS_3</th>\n",
       "      <th>CLS_4</th>\n",
       "      <th>CLS_5</th>\n",
       "      <th>CLS_6</th>\n",
       "      <th>CLS_7</th>\n",
       "      <th>CLS_8</th>\n",
       "      <th>CLS_9</th>\n",
       "      <th>...</th>\n",
       "      <th>CLS_374</th>\n",
       "      <th>CLS_375</th>\n",
       "      <th>CLS_376</th>\n",
       "      <th>CLS_377</th>\n",
       "      <th>CLS_378</th>\n",
       "      <th>CLS_379</th>\n",
       "      <th>CLS_380</th>\n",
       "      <th>CLS_381</th>\n",
       "      <th>CLS_382</th>\n",
       "      <th>CLS_383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.029546</td>\n",
       "      <td>-0.093779</td>\n",
       "      <td>0.035240</td>\n",
       "      <td>-0.029979</td>\n",
       "      <td>0.094636</td>\n",
       "      <td>0.060090</td>\n",
       "      <td>-0.011613</td>\n",
       "      <td>-0.011853</td>\n",
       "      <td>0.025654</td>\n",
       "      <td>0.051191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016053</td>\n",
       "      <td>0.055923</td>\n",
       "      <td>-0.015797</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.090052</td>\n",
       "      <td>0.084351</td>\n",
       "      <td>0.031153</td>\n",
       "      <td>0.027775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.029816</td>\n",
       "      <td>-0.066288</td>\n",
       "      <td>0.014838</td>\n",
       "      <td>-0.033962</td>\n",
       "      <td>0.069943</td>\n",
       "      <td>0.035519</td>\n",
       "      <td>-0.034434</td>\n",
       "      <td>-0.038205</td>\n",
       "      <td>0.018258</td>\n",
       "      <td>0.085273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024541</td>\n",
       "      <td>0.068040</td>\n",
       "      <td>-0.034506</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.047106</td>\n",
       "      <td>-0.011689</td>\n",
       "      <td>0.088033</td>\n",
       "      <td>0.080813</td>\n",
       "      <td>0.039639</td>\n",
       "      <td>0.052613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.020296</td>\n",
       "      <td>0.022372</td>\n",
       "      <td>-0.021896</td>\n",
       "      <td>-0.049170</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>-0.052099</td>\n",
       "      <td>-0.106442</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.093113</td>\n",
       "      <td>0.024022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.067288</td>\n",
       "      <td>0.048709</td>\n",
       "      <td>-0.005579</td>\n",
       "      <td>-0.033400</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>-0.011651</td>\n",
       "      <td>0.063168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.020296</td>\n",
       "      <td>0.022372</td>\n",
       "      <td>-0.021896</td>\n",
       "      <td>-0.049170</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>-0.052099</td>\n",
       "      <td>-0.106442</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.093113</td>\n",
       "      <td>0.024022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.067288</td>\n",
       "      <td>0.048709</td>\n",
       "      <td>-0.005579</td>\n",
       "      <td>-0.033400</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>-0.011651</td>\n",
       "      <td>0.063168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003252</td>\n",
       "      <td>0.024647</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>-0.039569</td>\n",
       "      <td>0.053363</td>\n",
       "      <td>-0.028918</td>\n",
       "      <td>-0.091161</td>\n",
       "      <td>-0.037333</td>\n",
       "      <td>0.029517</td>\n",
       "      <td>0.046394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>0.059754</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.005610</td>\n",
       "      <td>-0.011385</td>\n",
       "      <td>0.026064</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.067664</td>\n",
       "      <td>-0.016873</td>\n",
       "      <td>0.057681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CLS_0     CLS_1     CLS_2     CLS_3     CLS_4     CLS_5     CLS_6  \\\n",
       "0 -0.029546 -0.093779  0.035240 -0.029979  0.094636  0.060090 -0.011613   \n",
       "1 -0.029816 -0.066288  0.014838 -0.033962  0.069943  0.035519 -0.034434   \n",
       "2 -0.020296  0.022372 -0.021896 -0.049170  0.018540 -0.052099 -0.106442   \n",
       "3 -0.020296  0.022372 -0.021896 -0.049170  0.018540 -0.052099 -0.106442   \n",
       "4 -0.003252  0.024647  0.005446 -0.039569  0.053363 -0.028918 -0.091161   \n",
       "\n",
       "      CLS_7     CLS_8     CLS_9  ...   CLS_374   CLS_375   CLS_376   CLS_377  \\\n",
       "0 -0.011853  0.025654  0.051191  ...  0.016053  0.055923 -0.015797  0.009030   \n",
       "1 -0.038205  0.018258  0.085273  ...  0.024541  0.068040 -0.034506  0.004123   \n",
       "2  0.001228  0.093113  0.024022  ...  0.011164  0.067288  0.048709 -0.005579   \n",
       "3  0.001228  0.093113  0.024022  ...  0.011164  0.067288  0.048709 -0.005579   \n",
       "4 -0.037333  0.029517  0.046394  ...  0.024138  0.059754 -0.002547 -0.005610   \n",
       "\n",
       "    CLS_378   CLS_379   CLS_380   CLS_381   CLS_382   CLS_383  \n",
       "0  0.016759  0.004438  0.090052  0.084351  0.031153  0.027775  \n",
       "1  0.047106 -0.011689  0.088033  0.080813  0.039639  0.052613  \n",
       "2 -0.033400  0.029852  0.000440  0.026421 -0.011651  0.063168  \n",
       "3 -0.033400  0.029852  0.000440  0.026421 -0.011651  0.063168  \n",
       "4 -0.011385  0.026064  0.008469  0.067664 -0.016873  0.057681  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "cls_features.columns = [f\"CLS_{i}\" for i in range(cls_features.shape[1])]\n",
    "cls_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metadata_label</th>\n",
       "      <th>Metadata_cell_idx</th>\n",
       "      <th>Metadata_Time</th>\n",
       "      <th>CLS_0</th>\n",
       "      <th>CLS_1</th>\n",
       "      <th>CLS_2</th>\n",
       "      <th>CLS_3</th>\n",
       "      <th>CLS_4</th>\n",
       "      <th>CLS_5</th>\n",
       "      <th>CLS_6</th>\n",
       "      <th>...</th>\n",
       "      <th>CLS_374</th>\n",
       "      <th>CLS_375</th>\n",
       "      <th>CLS_376</th>\n",
       "      <th>CLS_377</th>\n",
       "      <th>CLS_378</th>\n",
       "      <th>CLS_379</th>\n",
       "      <th>CLS_380</th>\n",
       "      <th>CLS_381</th>\n",
       "      <th>CLS_382</th>\n",
       "      <th>CLS_383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.029546</td>\n",
       "      <td>-0.093779</td>\n",
       "      <td>0.035240</td>\n",
       "      <td>-0.029979</td>\n",
       "      <td>0.094636</td>\n",
       "      <td>0.060090</td>\n",
       "      <td>-0.011613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016053</td>\n",
       "      <td>0.055923</td>\n",
       "      <td>-0.015797</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.090052</td>\n",
       "      <td>0.084351</td>\n",
       "      <td>0.031153</td>\n",
       "      <td>0.027775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.029816</td>\n",
       "      <td>-0.066288</td>\n",
       "      <td>0.014838</td>\n",
       "      <td>-0.033962</td>\n",
       "      <td>0.069943</td>\n",
       "      <td>0.035519</td>\n",
       "      <td>-0.034434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024541</td>\n",
       "      <td>0.068040</td>\n",
       "      <td>-0.034506</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.047106</td>\n",
       "      <td>-0.011689</td>\n",
       "      <td>0.088033</td>\n",
       "      <td>0.080813</td>\n",
       "      <td>0.039639</td>\n",
       "      <td>0.052613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.020296</td>\n",
       "      <td>0.022372</td>\n",
       "      <td>-0.021896</td>\n",
       "      <td>-0.049170</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>-0.052099</td>\n",
       "      <td>-0.106442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.067288</td>\n",
       "      <td>0.048709</td>\n",
       "      <td>-0.005579</td>\n",
       "      <td>-0.033400</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>-0.011651</td>\n",
       "      <td>0.063168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-0.020296</td>\n",
       "      <td>0.022372</td>\n",
       "      <td>-0.021896</td>\n",
       "      <td>-0.049170</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>-0.052099</td>\n",
       "      <td>-0.106442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.067288</td>\n",
       "      <td>0.048709</td>\n",
       "      <td>-0.005579</td>\n",
       "      <td>-0.033400</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>-0.011651</td>\n",
       "      <td>0.063168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.003252</td>\n",
       "      <td>0.024647</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>-0.039569</td>\n",
       "      <td>0.053363</td>\n",
       "      <td>-0.028918</td>\n",
       "      <td>-0.091161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>0.059754</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.005610</td>\n",
       "      <td>-0.011385</td>\n",
       "      <td>0.026064</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.067664</td>\n",
       "      <td>-0.016873</td>\n",
       "      <td>0.057681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 387 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Metadata_label  Metadata_cell_idx  Metadata_Time     CLS_0     CLS_1  \\\n",
       "0             5.0            10006.0            0.0 -0.029546 -0.093779   \n",
       "1             5.0            10006.0            1.0 -0.029816 -0.066288   \n",
       "2             5.0            10006.0           10.0 -0.020296  0.022372   \n",
       "3             5.0            10006.0           11.0 -0.020296  0.022372   \n",
       "4             5.0            10006.0           12.0 -0.003252  0.024647   \n",
       "\n",
       "      CLS_2     CLS_3     CLS_4     CLS_5     CLS_6  ...   CLS_374   CLS_375  \\\n",
       "0  0.035240 -0.029979  0.094636  0.060090 -0.011613  ...  0.016053  0.055923   \n",
       "1  0.014838 -0.033962  0.069943  0.035519 -0.034434  ...  0.024541  0.068040   \n",
       "2 -0.021896 -0.049170  0.018540 -0.052099 -0.106442  ...  0.011164  0.067288   \n",
       "3 -0.021896 -0.049170  0.018540 -0.052099 -0.106442  ...  0.011164  0.067288   \n",
       "4  0.005446 -0.039569  0.053363 -0.028918 -0.091161  ...  0.024138  0.059754   \n",
       "\n",
       "    CLS_376   CLS_377   CLS_378   CLS_379   CLS_380   CLS_381   CLS_382  \\\n",
       "0 -0.015797  0.009030  0.016759  0.004438  0.090052  0.084351  0.031153   \n",
       "1 -0.034506  0.004123  0.047106 -0.011689  0.088033  0.080813  0.039639   \n",
       "2  0.048709 -0.005579 -0.033400  0.029852  0.000440  0.026421 -0.011651   \n",
       "3  0.048709 -0.005579 -0.033400  0.029852  0.000440  0.026421 -0.011651   \n",
       "4 -0.002547 -0.005610 -0.011385  0.026064  0.008469  0.067664 -0.016873   \n",
       "\n",
       "    CLS_383  \n",
       "0  0.027775  \n",
       "1  0.052613  \n",
       "2  0.063168  \n",
       "3  0.063168  \n",
       "4  0.057681  \n",
       "\n",
       "[5 rows x 387 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename the image paths columns\n",
    "image_paths.columns = [\"Metadata_image_paths\"]\n",
    "# make metadata columns for the image paths\n",
    "cls_features[\"Metadata_label\"] = image_paths[\"Metadata_image_paths\"].apply(\n",
    "    lambda x: pathlib.Path(x).stem.split(\"_\")[1]\n",
    ")\n",
    "cls_features[\"Metadata_cell_idx\"] = image_paths[\"Metadata_image_paths\"].apply(\n",
    "    lambda x: pathlib.Path(x).stem.split(\"_\")[3]\n",
    ")\n",
    "cls_features[\"Metadata_Time\"] = image_paths[\"Metadata_image_paths\"].apply(\n",
    "    lambda x: pathlib.Path(x).stem.split(\"_\")[5]\n",
    ")\n",
    "# reorder the columns so that the metadata columns are first\n",
    "cls_features = cls_features[\n",
    "    [\"Metadata_label\", \"Metadata_cell_idx\", \"Metadata_Time\"]\n",
    "    + cls_features.columns[:-3].tolist()\n",
    "]\n",
    "\n",
    "# make all columns floats\n",
    "cls_features = cls_features.astype(float)\n",
    "cls_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1500000, 384])\n",
      "torch.Size([60000, 25, 384])\n"
     ]
    }
   ],
   "source": [
    "cls_tensor = torch.tensor(cls_features.iloc[:, 3:].values)\n",
    "# reshape the data from (cell_idx, time, features) to (cell_idx, features*time)\n",
    "print(cls_tensor.shape)\n",
    "cls_tensor = torch.tensor(cls_features.iloc[:, 3:].values)\n",
    "cls_tensor = cls_tensor.reshape(\n",
    "    -1, cls_features.Metadata_Time.nunique(), cls_tensor.shape[1]\n",
    ")\n",
    "\n",
    "print(cls_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 25, 384]), torch.Size([20, 25, 384]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn\n",
    "\n",
    "# make a Dataset\n",
    "import torch.utils\n",
    "\n",
    "\n",
    "class CLSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, cls_features: pd.DataFrame):\n",
    "        super(CLSDataset, self).__init__()\n",
    "        self.cls_features = cls_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cls_features.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_timepoints(tensor: torch.Tensor, timepoints: int):\n",
    "        # mask one timepoint in the tensor\n",
    "        # random value from 0 to timepoints\n",
    "        random_timepoint = np.random.randint(0, timepoints)\n",
    "        tensor[random_timepoint, :] = 1\n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.cls_features[idx, :, :]\n",
    "        x = self.mask_timepoints(y, y.shape[0])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# make a DataLoader\n",
    "cls_dataset = CLSDataset(cls_tensor)\n",
    "cls_loader = torch.utils.data.DataLoader(cls_dataset, batch_size=20, shuffle=False)\n",
    "# get the first batch size\n",
    "x, y = next(iter(cls_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://medium.com/correll-lab/building-a-vision-transformer-model-from-scratch-a3054f707cc6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1422998/3781259385.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos / 10000 ** (2 * i / torch.tensor(d_model))\n",
      "/tmp/ipykernel_1422998/3781259385.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos / 10000 ** (2 * i / torch.tensor(d_model))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8520,  1.5166,  0.2156,  ...,  1.1039, -1.4358,  0.5494],\n",
       "         [-0.3311, -0.7266,  0.8207,  ..., -0.4056, -0.3778,  1.9836],\n",
       "         [ 0.8693, -0.0301,  0.8754,  ...,  0.8945, -0.3249,  0.6853],\n",
       "         ...,\n",
       "         [ 0.5532,  1.2587,  0.2955,  ...,  2.1656, -0.4710,  0.0662],\n",
       "         [ 1.2258,  0.7680, -0.2650,  ...,  0.1283,  0.5580,  0.5760],\n",
       "         [ 1.7366,  0.2619, -0.2478,  ...,  1.4762, -0.4329, -0.7781]],\n",
       "\n",
       "        [[-0.8520,  1.5166,  0.2156,  ...,  1.1039, -1.4358,  0.5494],\n",
       "         [-0.5047,  1.4123,  1.7736,  ...,  3.2340,  0.5619,  1.1527],\n",
       "         [ 1.1611, -0.7600, -2.4386,  ...,  3.1187,  0.8189,  3.4872],\n",
       "         ...,\n",
       "         [ 0.7750, -0.3959,  0.0059,  ...,  1.8952, -0.5980,  0.8607],\n",
       "         [-0.6186, -2.8388,  0.6824,  ..., -0.4546, -0.8062, -0.7220],\n",
       "         [-0.2989, -0.1119, -0.1014,  ...,  2.0180,  0.4916,  0.3415]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionWiseEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(1, 1, d_model)\n",
    "        )  # Classification Token\n",
    "\n",
    "        # Creating positional encoding\n",
    "        pe = torch.zeros(max_seq_length + 1, d_model)\n",
    "\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(d_model):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = torch.sin(\n",
    "                        pos / 10000 ** (2 * i / torch.tensor(d_model))\n",
    "                    )\n",
    "                else:\n",
    "                    pe[pos][i] = torch.cos(\n",
    "                        pos / 10000 ** (2 * i / torch.tensor(d_model))\n",
    "                    )\n",
    "\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand to have class token for every image in batch\n",
    "        tokens_batch = self.cls_token.expand(x.size()[0], -1, -1)\n",
    "\n",
    "        # Adding class tokens to the beginning of each embedding\n",
    "        x = torch.cat((tokens_batch, x), dim=1)\n",
    "\n",
    "        # Add positional encoding to embeddings\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# test the PositionWiseEncoding\n",
    "d_model = torch.tensor(512)\n",
    "max_seq_length = torch.tensor(10)\n",
    "pe = PositionWiseEncoding(d_model, max_seq_length)\n",
    "pe.forward(torch.randn(2, 10, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2291,  0.1659,  0.0525,  ...,  0.1605, -0.0658,  0.0952],\n",
       "         [-0.1823,  0.1036,  0.0826,  ...,  0.1731, -0.0188,  0.0398],\n",
       "         [-0.1861,  0.1028,  0.0512,  ...,  0.1523, -0.0127,  0.0357],\n",
       "         ...,\n",
       "         [-0.2443,  0.0841,  0.0678,  ...,  0.1335, -0.0594,  0.0773],\n",
       "         [-0.1849,  0.0549,  0.0582,  ...,  0.1972, -0.1228,  0.1137],\n",
       "         [-0.2479,  0.0503,  0.0762,  ...,  0.1961, -0.1098,  0.0595]],\n",
       "\n",
       "        [[ 0.3021,  0.2776, -0.1754,  ...,  0.1452, -0.0092, -0.0438],\n",
       "         [ 0.2084,  0.1718, -0.1744,  ...,  0.1735, -0.1209, -0.0229],\n",
       "         [ 0.2806,  0.2008, -0.1458,  ...,  0.1322,  0.0151,  0.0187],\n",
       "         ...,\n",
       "         [ 0.2828,  0.2494, -0.1780,  ...,  0.0848, -0.0567, -0.0185],\n",
       "         [ 0.2985,  0.2268, -0.1691,  ...,  0.0582, -0.0411, -0.0832],\n",
       "         [ 0.2444,  0.2720, -0.1666,  ...,  0.1067,  0.0453,  0.0076]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.query = nn.Linear(d_model, head_size)\n",
    "        self.key = nn.Linear(d_model, head_size)\n",
    "        self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Obtaining Queries, Keys, and Values\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Dot Product of Queries and Keys\n",
    "        attention = Q @ K.transpose(-2, -1)\n",
    "\n",
    "        # Scaling\n",
    "        attention = attention / (self.head_size**0.5)\n",
    "\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        attention = attention @ V\n",
    "\n",
    "        return attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = d_model // n_heads\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(d_model, self.head_size) for _ in range(n_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Combine attention heads\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "        out = self.W_o(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# test the MultiHeadAttention\n",
    "d_model = torch.tensor(512)\n",
    "n_heads = torch.tensor(8)\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "mha.forward(torch.randn(2, 10, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3648, -0.1755,  0.4912,  ...,  1.0293,  1.1186,  1.1497],\n",
       "         [ 0.3566, -1.5364,  0.3265,  ..., -0.0484, -0.8855, -0.4042],\n",
       "         [-0.6996,  1.4297,  0.1730,  ..., -0.8811, -0.4816,  3.0337],\n",
       "         ...,\n",
       "         [-0.4999,  0.1597,  0.8121,  ..., -0.0100,  0.1191, -0.3411],\n",
       "         [-1.3076,  0.4992, -0.5681,  ..., -1.3836, -0.3848, -1.2969],\n",
       "         [ 0.3112, -0.7330,  0.3809,  ..., -0.1236,  0.3230,  0.4748]],\n",
       "\n",
       "        [[-0.0469,  1.4134,  1.6866,  ...,  1.4317, -0.2477,  1.6590],\n",
       "         [-1.2577, -1.3425, -0.6988,  ...,  0.2587, -1.0133,  0.2153],\n",
       "         [-0.3154,  0.5306,  1.4096,  ...,  0.3559,  0.8954, -2.0120],\n",
       "         ...,\n",
       "         [ 1.6707, -1.3565, -0.4847,  ..., -1.4779,  0.0339, -2.5156],\n",
       "         [ 0.6411,  0.3048, -0.2843,  ..., -1.1491,  1.7195, -0.4898],\n",
       "         [-0.2715, -1.1387, -0.9141,  ..., -1.0430, -0.0797, -0.0657]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Sub-Layer 1 Normalization\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        # Sub-Layer 2 Normalization\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Multilayer Perception\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * r_mlp, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual Connection After Sub-Layer 1\n",
    "        out = x + self.mha(self.ln1(x))\n",
    "\n",
    "        # Residual Connection After Sub-Layer 2\n",
    "        out = out + self.mlp(self.ln2(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# test the TransformerEncoder\n",
    "d_model = 384\n",
    "n_heads = 8\n",
    "r_mlp = 4\n",
    "te = TransformerEncoder(d_model, n_heads, r_mlp)\n",
    "te.forward(torch.randn(2, 10, 384, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0514,  1.1926,  1.1813,  ..., -0.0934,  0.1782,  1.6328],\n",
       "         [ 0.6155, -1.1070,  0.0936,  ...,  0.5635,  0.6225, -1.0014],\n",
       "         [ 1.1637,  0.1411,  0.7319,  ..., -2.5400, -0.5850, -0.0731],\n",
       "         ...,\n",
       "         [ 1.9988,  1.7400,  0.4203,  ...,  1.1801, -0.6749, -0.9886],\n",
       "         [ 1.6551, -0.4430,  0.1328,  ...,  1.1269, -0.4040,  0.7960],\n",
       "         [ 0.0944, -1.2211,  0.0543,  ...,  0.1675,  0.3382, -1.0189]],\n",
       "\n",
       "        [[ 1.5985,  2.0573,  1.6586,  ..., -1.7370, -1.4980,  0.7284],\n",
       "         [ 1.7054,  0.8217, -0.3673,  ...,  0.4460, -0.0578,  1.9114],\n",
       "         [-0.3797, -1.0880,  0.5592,  ...,  0.1785,  0.2280, -0.0547],\n",
       "         ...,\n",
       "         [ 2.2847,  1.9717, -0.6047,  ...,  0.4749, -1.1031, -0.1852],\n",
       "         [ 2.2427, -0.9110, -0.8853,  ..., -2.0369,  1.4158,  1.3443],\n",
       "         [-0.0526,  0.6650, -0.0909,  ..., -1.1365, -0.9365,  0.2878]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, d_model, n_classes, n_heads, n_layers, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_model = d_model  # Dimensionality of model\n",
    "        self.n_classes = n_classes  # Number of classes\n",
    "        self.n_heads = n_heads  # Number of attention heads\n",
    "        self.seq_len = seq_len  # Sequence length\n",
    "\n",
    "        self.positional_encoding = PositionWiseEncoding(self.d_model, self.seq_len)\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            *[TransformerEncoder(self.d_model, self.n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # Classification MLP\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.n_classes), nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # x = self.classifier(x[:,0])\n",
    "        # remove the class token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# test the Transformer\n",
    "d_model = 384\n",
    "n_classes = 10\n",
    "n_heads = 8\n",
    "n_layers = 4\n",
    "seq_len = 10\n",
    "tt = TemporalTransformer(d_model, n_classes, n_heads, n_layers, seq_len)\n",
    "tt.forward(torch.randn(2, 10, 384))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test to see if the model works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 384\n",
    "n_classes = 10\n",
    "n_heads = 12\n",
    "n_layers = 8\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "alpha = 0.05\n",
    "\n",
    "seq_len = 25\n",
    "# training data loader is cls_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 3090 Ti)\n",
      "Epoch 1/50 loss: 15785.364\n",
      "Epoch 2/50 loss: 9205543.888\n",
      "Epoch 3/50 loss: 13335215.535\n",
      "Epoch 4/50 loss: 125900.951\n",
      "Epoch 5/50 loss: 8607415.818\n",
      "Epoch 6/50 loss: 139964.030\n",
      "Epoch 7/50 loss: 3268688.919\n",
      "Epoch 8/50 loss: 2133496.189\n",
      "Epoch 9/50 loss: 5145497.964\n",
      "Epoch 10/50 loss: 68212.372\n",
      "Epoch 11/50 loss: 87717.813\n",
      "Epoch 12/50 loss: 7763610.521\n",
      "Epoch 13/50 loss: 150699.345\n",
      "Epoch 14/50 loss: 5725287.957\n",
      "Epoch 15/50 loss: 677967.750\n",
      "Epoch 16/50 loss: 1463432.424\n",
      "Epoch 17/50 loss: 25734.189\n",
      "Epoch 18/50 loss: 4526396.255\n",
      "Epoch 19/50 loss: 74826.618\n",
      "Epoch 20/50 loss: 1526481.728\n",
      "Epoch 21/50 loss: 16807.412\n",
      "Epoch 22/50 loss: 28945.068\n",
      "Epoch 23/50 loss: 4443634.065\n",
      "Epoch 24/50 loss: 82033.652\n",
      "Epoch 25/50 loss: 2695550.868\n",
      "Epoch 26/50 loss: 198415.600\n",
      "Epoch 27/50 loss: 137859.083\n",
      "Epoch 28/50 loss: 50971.955\n",
      "Epoch 29/50 loss: 1297717.178\n",
      "Epoch 30/50 loss: 54474.891\n",
      "Epoch 31/50 loss: 1677355.889\n",
      "Epoch 32/50 loss: 50343.792\n",
      "Epoch 33/50 loss: 34976.386\n",
      "Epoch 34/50 loss: 2321612.836\n",
      "Epoch 35/50 loss: 37578.811\n",
      "Epoch 36/50 loss: 5406619.126\n",
      "Epoch 37/50 loss: 141827.885\n",
      "Epoch 38/50 loss: 925300.382\n",
      "Epoch 39/50 loss: 34619500.178\n",
      "Epoch 40/50 loss: 928314.523\n",
      "Epoch 41/50 loss: 399229.213\n",
      "Epoch 42/50 loss: 20835931.619\n",
      "Epoch 43/50 loss: 826151.109\n",
      "Epoch 44/50 loss: 3824521.195\n",
      "Epoch 45/50 loss: 126687.515\n",
      "Epoch 46/50 loss: 19320161.153\n",
      "Epoch 47/50 loss: 431142.574\n",
      "Epoch 48/50 loss: 314015.499\n",
      "Epoch 49/50 loss: 6638481.472\n",
      "Epoch 50/50 loss: 241874.745\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\n",
    "    \"Using device: \",\n",
    "    device,\n",
    "    f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
    ")\n",
    "\n",
    "transformer = TemporalTransformer(d_model, n_classes, n_heads, n_layers, seq_len).to(\n",
    "    device\n",
    ")\n",
    "# import adam\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(transformer.parameters(), lr=alpha)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    training_loss = 0.0\n",
    "    for i, data in enumerate(cls_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = transformer(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} loss: {training_loss  / len(cls_loader) :.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reconstructed = []\n",
    "list_of_original = []\n",
    "with torch.no_grad():\n",
    "    for data in cls_loader:\n",
    "        x, y = data\n",
    "        x, y = x.to(device).float(), y.to(device).float()\n",
    "\n",
    "        # reshape (B, T, d) -> (B*T, d)\n",
    "        x = transformer(x)\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        y = y.reshape(-1, y.shape[-1])\n",
    "        list_of_reconstructed.append(x.cpu().numpy())\n",
    "        list_of_original.append(y.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLS_0</th>\n",
       "      <th>CLS_1</th>\n",
       "      <th>CLS_2</th>\n",
       "      <th>CLS_3</th>\n",
       "      <th>CLS_4</th>\n",
       "      <th>CLS_5</th>\n",
       "      <th>CLS_6</th>\n",
       "      <th>CLS_7</th>\n",
       "      <th>CLS_8</th>\n",
       "      <th>CLS_9</th>\n",
       "      <th>...</th>\n",
       "      <th>CLS_374</th>\n",
       "      <th>CLS_375</th>\n",
       "      <th>CLS_376</th>\n",
       "      <th>CLS_377</th>\n",
       "      <th>CLS_378</th>\n",
       "      <th>CLS_379</th>\n",
       "      <th>CLS_380</th>\n",
       "      <th>CLS_381</th>\n",
       "      <th>CLS_382</th>\n",
       "      <th>CLS_383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.759766</td>\n",
       "      <td>645.414062</td>\n",
       "      <td>-275.111816</td>\n",
       "      <td>-30.500000</td>\n",
       "      <td>-329.777344</td>\n",
       "      <td>239.705078</td>\n",
       "      <td>350.839844</td>\n",
       "      <td>119.054688</td>\n",
       "      <td>-2704.781250</td>\n",
       "      <td>267.146484</td>\n",
       "      <td>...</td>\n",
       "      <td>145.539062</td>\n",
       "      <td>504.338196</td>\n",
       "      <td>432.873047</td>\n",
       "      <td>285.309082</td>\n",
       "      <td>202.203979</td>\n",
       "      <td>242.397461</td>\n",
       "      <td>-533.054688</td>\n",
       "      <td>-419.226562</td>\n",
       "      <td>666.085938</td>\n",
       "      <td>-150.983398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.878906</td>\n",
       "      <td>644.570312</td>\n",
       "      <td>-274.915527</td>\n",
       "      <td>-31.322266</td>\n",
       "      <td>-329.529785</td>\n",
       "      <td>239.039062</td>\n",
       "      <td>351.185547</td>\n",
       "      <td>118.449219</td>\n",
       "      <td>-2704.484375</td>\n",
       "      <td>266.621094</td>\n",
       "      <td>...</td>\n",
       "      <td>145.570312</td>\n",
       "      <td>504.352936</td>\n",
       "      <td>432.884766</td>\n",
       "      <td>285.304810</td>\n",
       "      <td>202.225220</td>\n",
       "      <td>242.434570</td>\n",
       "      <td>-533.062500</td>\n",
       "      <td>-419.242188</td>\n",
       "      <td>666.074219</td>\n",
       "      <td>-150.958984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.208984</td>\n",
       "      <td>643.980469</td>\n",
       "      <td>-275.475586</td>\n",
       "      <td>-32.025391</td>\n",
       "      <td>-329.935547</td>\n",
       "      <td>238.355469</td>\n",
       "      <td>350.964844</td>\n",
       "      <td>117.761719</td>\n",
       "      <td>-2704.609375</td>\n",
       "      <td>265.980469</td>\n",
       "      <td>...</td>\n",
       "      <td>145.611328</td>\n",
       "      <td>504.379883</td>\n",
       "      <td>432.906250</td>\n",
       "      <td>285.311890</td>\n",
       "      <td>202.214233</td>\n",
       "      <td>242.489258</td>\n",
       "      <td>-533.078125</td>\n",
       "      <td>-419.242188</td>\n",
       "      <td>666.089844</td>\n",
       "      <td>-150.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.384766</td>\n",
       "      <td>644.187500</td>\n",
       "      <td>-276.349609</td>\n",
       "      <td>-32.126953</td>\n",
       "      <td>-330.697266</td>\n",
       "      <td>238.052734</td>\n",
       "      <td>350.333984</td>\n",
       "      <td>117.320312</td>\n",
       "      <td>-2705.031250</td>\n",
       "      <td>265.476562</td>\n",
       "      <td>...</td>\n",
       "      <td>145.650391</td>\n",
       "      <td>504.393127</td>\n",
       "      <td>432.925781</td>\n",
       "      <td>285.305176</td>\n",
       "      <td>202.216858</td>\n",
       "      <td>242.533203</td>\n",
       "      <td>-533.070312</td>\n",
       "      <td>-419.230469</td>\n",
       "      <td>666.082031</td>\n",
       "      <td>-150.903320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.189453</td>\n",
       "      <td>644.988281</td>\n",
       "      <td>-276.873535</td>\n",
       "      <td>-31.544922</td>\n",
       "      <td>-331.369629</td>\n",
       "      <td>238.320312</td>\n",
       "      <td>349.587891</td>\n",
       "      <td>117.367188</td>\n",
       "      <td>-2705.671875</td>\n",
       "      <td>265.353516</td>\n",
       "      <td>...</td>\n",
       "      <td>145.642578</td>\n",
       "      <td>504.394104</td>\n",
       "      <td>432.910156</td>\n",
       "      <td>285.307373</td>\n",
       "      <td>202.191589</td>\n",
       "      <td>242.540039</td>\n",
       "      <td>-533.070312</td>\n",
       "      <td>-419.214844</td>\n",
       "      <td>666.089844</td>\n",
       "      <td>-150.915039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CLS_0       CLS_1       CLS_2      CLS_3       CLS_4       CLS_5  \\\n",
       "0  51.759766  645.414062 -275.111816 -30.500000 -329.777344  239.705078   \n",
       "1  51.878906  644.570312 -274.915527 -31.322266 -329.529785  239.039062   \n",
       "2  51.208984  643.980469 -275.475586 -32.025391 -329.935547  238.355469   \n",
       "3  50.384766  644.187500 -276.349609 -32.126953 -330.697266  238.052734   \n",
       "4  50.189453  644.988281 -276.873535 -31.544922 -331.369629  238.320312   \n",
       "\n",
       "        CLS_6       CLS_7        CLS_8       CLS_9  ...     CLS_374  \\\n",
       "0  350.839844  119.054688 -2704.781250  267.146484  ...  145.539062   \n",
       "1  351.185547  118.449219 -2704.484375  266.621094  ...  145.570312   \n",
       "2  350.964844  117.761719 -2704.609375  265.980469  ...  145.611328   \n",
       "3  350.333984  117.320312 -2705.031250  265.476562  ...  145.650391   \n",
       "4  349.587891  117.367188 -2705.671875  265.353516  ...  145.642578   \n",
       "\n",
       "      CLS_375     CLS_376     CLS_377     CLS_378     CLS_379     CLS_380  \\\n",
       "0  504.338196  432.873047  285.309082  202.203979  242.397461 -533.054688   \n",
       "1  504.352936  432.884766  285.304810  202.225220  242.434570 -533.062500   \n",
       "2  504.379883  432.906250  285.311890  202.214233  242.489258 -533.078125   \n",
       "3  504.393127  432.925781  285.305176  202.216858  242.533203 -533.070312   \n",
       "4  504.394104  432.910156  285.307373  202.191589  242.540039 -533.070312   \n",
       "\n",
       "      CLS_381     CLS_382     CLS_383  \n",
       "0 -419.226562  666.085938 -150.983398  \n",
       "1 -419.242188  666.074219 -150.958984  \n",
       "2 -419.242188  666.089844 -150.937500  \n",
       "3 -419.230469  666.082031 -150.903320  \n",
       "4 -419.214844  666.089844 -150.915039  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make two different dfs for the reconstructed and original data\n",
    "reconstructed_df = pd.DataFrame(np.concatenate(list_of_reconstructed))\n",
    "original_df = pd.DataFrame(np.concatenate(list_of_original))\n",
    "\n",
    "# add the metadata columns\n",
    "reconstructed_df = pd.concat([cls_features.iloc[:, :3], reconstructed_df], axis=1)\n",
    "original_df = pd.concat([cls_features.iloc[:, :3], original_df], axis=1)\n",
    "\n",
    "# add label for reconstructed and original\n",
    "reconstructed_df[\"Metadata_reconstructed\"] = \"reconstructed_df\"\n",
    "original_df[\"Metadata_reconstructed\"] = \"original_df\"\n",
    "\n",
    "# combine the two dataframes\n",
    "combined_df = pd.concat([reconstructed_df, original_df])\n",
    "combined_df.head()\n",
    "# rename columns if int columns\n",
    "combined_df.rename(\n",
    "    columns={\n",
    "        col: f\"CLS_{col}\" if isinstance(col, int) else col\n",
    "        for col in combined_df.columns\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "# metadata columns\n",
    "metadata_columns = [col for col in combined_df.columns if \"Metadata\" in col]\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "features_df = combined_df.drop(metadata_columns, axis=1)\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lippincm/miniforge3/envs/transformers_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lippincm/miniforge3/envs/transformers_env/lib/python3.12/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "# umap\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, n_components=2, metric=\"euclidean\")\n",
    "embedding = reducer.fit_transform(features_df)\n",
    "embedding_df = pd.DataFrame(embedding, columns=[\"UMAP1\", \"UMAP2\"])\n",
    "embedding_df[\"Metadata_reconstructed\"] = combined_df[\"Metadata_reconstructed\"]\n",
    "embedding_df[\"Metadata_label\"] = combined_df[\"Metadata_label\"]\n",
    "embedding_df[\"Metadata_cell_idx\"] = combined_df[\"Metadata_cell_idx\"]\n",
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# randomize the rows for plotting\n",
    "embedding_df = embedding_df.sample(frac=1)\n",
    "# two subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "sns.scatterplot(\n",
    "    data=embedding_df, x=\"UMAP1\", y=\"UMAP2\", hue=\"Metadata_reconstructed\", ax=ax[0]\n",
    ")\n",
    "sns.scatterplot(data=embedding_df, x=\"UMAP1\", y=\"UMAP2\", hue=\"Metadata_label\", ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
